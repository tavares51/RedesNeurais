{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Centro Universitário Facens<br/></center>\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Ciência de Dados</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Renato Moraes Silva</center></font>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Perceptron multicamadas: implementação do zero</center>\n",
    "\n",
    "Neste exercício, você irá implementar uma rede neural artificial com *backpropagation* que será aplicada na tarefa de reconhecimento de dígitos manuscritos (Figura 1). \n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 0px; float: center;\">\n",
    "    <img src=\"figs/digitos.png\"  style=\"height:400px;\"/> \n",
    "    <center><em>Figura 1. Amostras do conjunto de dados.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "O conjunto que você utilizará será de digitos manuscritos. Cada imagem tem dimensão de 28 x 28 pixels e cada pixel é representado por um valor inteiro com a intensidade do tom de cinza naquela região.\n",
    "\n",
    "O conjunto de dados contém 5.000 amostras dos dígitos de 0 a 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruções\n",
    "----------\n",
    "\n",
    "Neste exercício, você precisará completar as seguintes funções:\n",
    "\n",
    "* `forwardPropagation()`\n",
    "* `funcaoCusto()`\n",
    "* `funcaoCusto_reg()`\n",
    "* `sigmoidGradient()`\n",
    "* `predicao()`\n",
    "\n",
    "Você não poderá criar nenhuma outra função. Apenas altere as rotinas fornecidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e visualizando os dados\n",
    "\n",
    "Vamos descompactar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "import os # importa a biblioteca para tarefas relacionadas ao sistema operacional\n",
    "\n",
    "# biblioteca para abrir as imagens\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "from zipfile import ZipFile # biblioteca para arquivoz zipados\n",
    "\n",
    "z = ZipFile('dados/MNIST.zip', 'r')\n",
    "z.extractall('dados/')\n",
    "z.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos carregar essa base de dados e guardar dos dados e as classes em uma lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDataset(path):\n",
    "    # percorre as pastas e coleta as imagens \n",
    "    labelList = []\n",
    "    imgList = []\n",
    "    for i in range(0,10):\n",
    "\n",
    "        print(\"Folder: %d\" %i)\n",
    "         \n",
    "        # coleta os nomes dos arquivos da pasta\n",
    "        files = os.listdir(path + str(i) )\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            # abre a imagem\n",
    "            img = cv2.imread(path + str(i) + '/' + file, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            imgList.append(img)\n",
    "            \n",
    "            # guarda a classe da imagem\n",
    "            labelList.append(i)\n",
    "    \n",
    "    return imgList, labelList\n",
    "\n",
    "# Caminho dos arquivos\n",
    "path = \"dados/MNIST/\"\n",
    "\n",
    "imgList, labelList = importDataset(path)       \n",
    "    \n",
    "print('\\nQuantidade de imagens: ', len(imgList))\n",
    "\n",
    "print('\\nDimensão de cada imagem: ', imgList[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As imagens e as classes foram guardadas em uma lista. Iremos converter as imagens em uma matriz, onde cada linha representará uma imagem. Converta também a lista de classes em um vetor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variável que deverá recer as imagens \n",
    "X_img = None\n",
    "\n",
    "# variável que irá receber as classes\n",
    "Y = None\n",
    "\n",
    "for i, img in enumerate(imgList):\n",
    "    imgList[i] = np.ravel( imgList[i] )\n",
    "                          \n",
    "\n",
    "X_img = np.asarray( imgList )\n",
    "Y = np.asarray( labelList )\n",
    "        \n",
    "print('\\nDimensao de X: ', X_img.shape)\n",
    "\n",
    "print('\\nDimensao de Y: ', Y.shape)\n",
    "\n",
    "print('\\nClasses do problema: ', np.unique(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tons de cinza são representados por valores entre 0 e 255. Esse formato de valores não é adequado para métodos de classificação baseados em otimização. Por isso, você deve normalizar esses valores para o intervalo entre 0 e 1. Para isso, basta dividir por 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variável que deverá recer as imagens normalizadas\n",
    "X = None\n",
    "\n",
    "X = X_img/255.0\n",
    "\n",
    "print('Maior valor de X: %1.10f' %np.max(X) )\n",
    "print('Menor valor de X: %1.10f' %np.min(X) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar aleatoriamente 100 amostras da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizaDados(X):\n",
    "    \n",
    "    # Calcula numero de linhas e colunas\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # calcula a largura das imagens\n",
    "    example_width = int(round(np.sqrt(n)) )\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Calcula numero de itens que serao exibidos\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, axs = plt.subplots(display_rows, display_cols, figsize=(7, 7))\n",
    "    \n",
    "    k = 0 # índice da imagem\n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            \n",
    "            new_X = np.reshape( np.ravel(X[k,:]), (example_width, example_height) )\n",
    "\n",
    "            axs[i,j].imshow(new_X, cmap='gray'); \n",
    "            axs[i,j].axis('off')\n",
    "            \n",
    "            k += 1\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "idx_perm = np.random.permutation( range(X.shape[0]) )\n",
    "visualizaDados( X[idx_perm[0:100],:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, iremos separar os dados em 80% para treinamento e 20% para teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout(X, Y, porcTrain = 0.8, randomSeed = 10):\n",
    "    \n",
    "    # total de dados\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # total de dados para treino\n",
    "    qtdTreino = int(m * porcTrain)\n",
    "    \n",
    "    # embaralha os indices\n",
    "    indicesEmbaralhados = np.random.RandomState(randomSeed).permutation(m) \n",
    "    \n",
    "    # embaralha os dados\n",
    "    X_shuffled = X[ indicesEmbaralhados, : ]\n",
    "    Y_shuffled = Y[ indicesEmbaralhados]\n",
    "    \n",
    "    # separa em treino e teste\n",
    "    X_train = X_shuffled[ 0:qtdTreino,: ]\n",
    "    Y_train = Y_shuffled[ 0:qtdTreino ]\n",
    "    \n",
    "    X_test = X_shuffled[ qtdTreino:,: ]\n",
    "    Y_test = Y_shuffled[ qtdTreino: ]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# separa em treino e teste\n",
    "X_train, X_test, Y_train, Y_test = holdout(X, Y, porcTrain = 0.8, randomSeed = 10)\n",
    "\n",
    "print('Qtd. dados de treinamento: %d (%1.2f%%)' %(X_train.shape[0], (X_train.shape[0]/X.shape[0])*100) )\n",
    "print('Qtd. de dados de teste: %d (%1.2f%%)' %(X_test.shape[0], (X_test.shape[0]/X.shape[0])*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando os parâmetros\n",
    "\n",
    "A rede neural proposta para este exercício tem 3 camadas: uma camada de entrada, uma camada oculta e uma camada de saída. É importante lembrar que a camada de entrada possui 784 neurônios que corresponde ao total de pixels de cada imagem (sem considerar o *bias*, sempre +1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos inicializar os parâmetros mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 784  # 28x28 dimensao das imagens de entrada\n",
    "hidden_layer_size = 25   # 25 neuronios na camada oculta\n",
    "num_labels = 10          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "\n",
    "print('\\nCarregando informações sobre a arquitetura da rede\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o problema que estamos trabalhando é multiclasse, a camada de saída terá $K$ neurônios, onde $K$ é a quantidade de classes. O vetor de saída precisa ser criado a partir da classe original, tornando-se compatível com a rede neural, ou seja, espera-se vetores com $|C|$ (onde, $|C|$ é o número de classes) posições contendo 1 para o elemento referente à classe esperada e 0 nos demais elementos. Por exemplo, seja 5 o rótulo de determinada amostra, o vetor $Y$ correspondente terá 1 na posição $y_5$ e 0 nas demais posições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte as classes para o formato binário\n",
    "def converteBinario(Y, num_labels):\n",
    "    m = len(Y)\n",
    "    \n",
    "    # variavel que irá ser retornada\n",
    "    Y_bin = None\n",
    "    \n",
    "    # (1) Converta em um vetor binário: 1 para o índice que representa a \n",
    "    #     classe atual, enquanto atribui 0 aos demais. \n",
    "\n",
    "    Y_bin = np.zeros([m,num_labels], dtype=int)\n",
    "    for i in range(m):\n",
    "        Y_bin[i,Y[i]] = 1\n",
    "        \n",
    "    return Y_bin\n",
    "\n",
    "# chama a funca que ira converter em binario\n",
    "Y_trainBin = converteBinario(Y_train, num_labels)\n",
    "\n",
    "print(\"Cinco primeiras classes: \")\n",
    "print(Y_trainBin[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos criar os pesos iniciais que serão usados na rede neural. Eles podem ser definidos aleatoriamente, com valores entre 0 e 1. Depois, a rede irá melhorando eles durante o aprendizado. Para que toda a execução gere o mesmo resultado, vamos usar uma semente para a função de geração de números aleatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializaPesosAleatorios(inputSize, outputSize, randomSeed = 10):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    inputSize (conexoes de entrada) e outputSize (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [outputSize, 1 + inputSize]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "    \n",
    "    # variável que deve ser retornada\n",
    "    W = None\n",
    "    \n",
    "    # parâmetro usado para controlar a escala dos pesos iniciais\n",
    "    epsilon_init = 0.12 \n",
    "\n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(outputSize, 1 + inputSize) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "         W = np.random.RandomState().rand( outputSize, 1+inputSize ) * 2 * epsilon_init - epsilon_init\n",
    "             \n",
    "    return W\n",
    "\n",
    "initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "print('\\nParametros inicializados com sucesso!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcula o custo (*Feedforward*)\n",
    "\n",
    "Você precisará implementar a função de custo e gradiente para a rede neural. Mas, antes disso é necessário computar a saída da rede neural ($h_\\Theta(x^{(i)})$) usando a etapa *forward propagation*. Nessa etapa, iremos usar a função de ativação sigmoidal: $\\displaystyle g(z) = \\frac{1}{1 + e^{-z}}$. Portanto, precisamos implementá-la primeiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "    \n",
    "    # variável que receberá o valor da função sigmoidal\n",
    "    sig = None\n",
    "    \n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return sig\n",
    "\n",
    "# testando a função sigmoidal\n",
    "z = sigmoid(0)\n",
    "print('sigmoid de 0 = %1.6f' %(z))\n",
    "\n",
    "z = sigmoid(10)\n",
    "print('sigmoid de 10 = %1.6f' %(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que implementamos a função de ativação sigmoidal, podemos usá-la para computar a resposta (ativação) de cada neurônio da camada de saída: $h_\\Theta(x^{(i)})_k$ = $a^{(3)}_k$, onde $k$ é o índice da $k$-ésima unidade de saída.\n",
    "\n",
    "Usando multiplicação matricial, a etapa *forward propagation* pode ser resumida da seguinte forma:\n",
    "\n",
    "$a^{(1)}  = x  \\text{ } \\left(\\text{adicionar } x_0 \\right)$ \n",
    "\n",
    "\n",
    "$z^{(2)}  =  \\Theta^{(1)} a^{(1)}$\n",
    "\n",
    "\n",
    "$a^{(2)}  = g \\left(z^{(2)}\\right)  \\text{ } \\left(\\text{adicionar } a_0^{(2)} \\right)$ \n",
    "\n",
    "\n",
    "$z^{(3)}  =  \\Theta^{(2)} a^{(2)} $\n",
    "\n",
    "\n",
    "$a^{(3)} = h_\\Theta(x) = g\\left(z^{(3)}\\right)$ \n",
    "\n",
    "\n",
    "Agora, você deve completar a função que executa a etapa *forward propagation*. O valor retornado por essa etapa ($h_\\Theta(x)$), será usado depois para obter o custo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropagation(X, Theta1, Theta2):\n",
    "    '''\n",
    "    Calcula o custo da rede neural\n",
    "    voltada para tarefa de classificacao.\n",
    "\n",
    "    Parametros\n",
    "    ----------   \n",
    "    X : matriz com os dados de treinamento\n",
    "    Theta1: pesos que ligam a camada de entrada à camada intermediária\n",
    "    Theta2: pesos que ligam a camada intermediária à camada de saída\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    hx: saída da rede\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # estrai a quantidade de classes\n",
    "    num_labels = len(np.unique(Y))\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # tamanho da camada intermediaria\n",
    "    hidden_layer_size = Theta1.shape[0]\n",
    "     \n",
    "    # Inicializa a variável que deve ser retornada corretamente\n",
    "    # Cada dado em X irá gerar uma resposta para cada uma das classes\n",
    "    hx = np.zeros([m,num_labels])\n",
    "    \n",
    "    # variáveis que você terá que retornar\n",
    "    # --------------------------------------\n",
    "    # \"z\" da segunda camada \n",
    "    z2 = np.zeros([m,hidden_layer_size])\n",
    "    \n",
    "    # \"a\" da segunda camada\n",
    "    a2 = np.zeros([m,hidden_layer_size+1])   \n",
    "    \n",
    "    # \"z\" da terceira camada\n",
    "    z3 = np.zeros([m,num_labels])\n",
    "    \n",
    "    # \"a\" da terceira camada (o mesmo que hx)\n",
    "    # cada dado em X irá gerar uma resposta para cada uma das classes\n",
    "    hx = np.zeros([m,num_labels]) \n",
    "    # --------------------------------------\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #\n",
    "    # (1) Execute a etapa de forward propagation para obter a resposta da rede\n",
    "    #      para cada dado em X em relação a cada uma das classes\n",
    "    # (2) Use a função hstack do numpy para acrescentar uma coluna de 1s no X\n",
    "    #     e formar a1\n",
    "    # (3) Diferente dos slides, as amostra em X estão nas linhas e os atributos\n",
    "    #     são as colunas. Portanto, para multiplicar pelos thetas use: X*theta.T\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return z2, a2, z3, hx\n",
    "\n",
    "print('\\nEtapa forward propagation  ...\\n')\n",
    "\n",
    "z2, a2, z3, hx = forwardPropagation(X_train, initial_Theta1, initial_Theta2)\n",
    "\n",
    "print('Saídas obtida para o primeiro dado em cada uma das classes: ')\n",
    "for i in range( num_labels ):\n",
    "    print('\\tClasse %d -- hx: %1.7f' %(i+1, hx[0,i]))\n",
    "    \n",
    "print('\\n\\nAs saídas esperadas são, aproximadamente, as seguintes:') \n",
    "print('\\tClasse 1 -- hx: 0.5190055') \n",
    "print('\\tClasse 2 -- hx: 0.4760282') \n",
    "print('\\tClasse 3 -- hx: 0.5697258') \n",
    "print('\\tClasse 4 -- hx: 0.4565804') \n",
    "print('\\tClasse 5 -- hx: 0.5227219') \n",
    "print('\\tClasse 6 -- hx: 0.5821649') \n",
    "print('\\tClasse 7 -- hx: 0.5725757') \n",
    "print('\\tClasse 8 -- hx: 0.5066974') \n",
    "print('\\tClasse 9 -- hx: 0.6033554') \n",
    "print('\\tClasse 10 -- hx: 0.4879005') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos usar $h_\\Theta(x^{(i)})$ computado acima para calcular o custo da rede neural. \n",
    "\n",
    "A função de custo (sem regularização) é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "Na função $J$, a constante $K$ representa o número de classes. Assim, $h_\\Theta(x^{(i)})_k$ corresponde à ativação (valor de saída) da $k$-ésima unidade de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto(X, Y, Theta1, Theta2):\n",
    "    '''\n",
    "    Calcula o custo da rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao.\n",
    "\n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "\n",
    "    Parametros\n",
    "    ----------   \n",
    "    X : matriz com os dados de treinamento\n",
    "    y : vetor com as classes dos dados de treinamento\n",
    "    Theta1: pesos que ligam a camada de entrada à camada intermediária\n",
    "    Theta2: pesos que ligam a camada intermediária à camada de saída\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    J: valor do custo\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # estrai a quantidade de classes\n",
    "    num_labels = len(np.unique(Y))\n",
    "        \n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "      \n",
    "    # Usa a função forwardPropagation feita anteriormente  \n",
    "    # Nesse exercício, você precisará usar apenas o hx retornado\n",
    "    # pela função forwardPropagation \n",
    "    z2, a2, z3, hx = forwardPropagation(X, Theta1, Theta2)\n",
    "\n",
    "    # inicializa o custo que você precisa calcular e retornar\n",
    "    J = 0; \n",
    "    \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    #\n",
    "    # (1): use as saídas da rede (hx) obtidas com a função forwardPropagation e as classes\n",
    "    #      reais binarizadas (Y) para calcular o custo \n",
    "    #\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J, z2, a2, z3, hx\n",
    "\n",
    "\n",
    "\n",
    "print('\\nFuncao de custo sem regularizacao ...\\n')\n",
    "\n",
    "J, z2, a2, z3, hx = funcaoCusto(X_train, Y_trainBin, initial_Theta1, initial_Theta2)\n",
    "\n",
    "print('Custo obtido: %1.6f ' %J)\n",
    "\n",
    "print('\\nCusto esperado: 7.603099')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularização\n",
    "\n",
    "A função de custo com regularização é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "$$ + \\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left(\\Theta^{(1)}_{j,k}\\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left(\\Theta^{(2)}_{j,k}\\right)^2\\right]$$\n",
    "\n",
    "É importante que a regularização não seja aplicada a termos relacionados aos *bias*. Neste contexto, estes termos estão na primeira coluna de cada matriz $\\Theta^{(1)}$ e $\\Theta^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, complete a nova função de custo aplicando regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_reg(X, Y, Theta1, Theta2, vLambda):\n",
    "    '''\n",
    "    Calcula o custo da rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao.\n",
    "\n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "\n",
    "    Parametros\n",
    "    ----------   \n",
    "    X : matriz com os dados de treinamento\n",
    "    y : vetor com as classes dos dados de treinamento\n",
    "    Theta1: pesos que ligam a camada de entrada à camada intermediária\n",
    "    Theta2: pesos que ligam a camada intermediária à camada de saída\n",
    "    vLambda: parametro de regularizacao\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    J: valor do custo\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # estrai a quantidade de classes\n",
    "    num_labels = len(np.unique(Y))\n",
    "        \n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "        \n",
    "    # Usa a função forwardPropagation feita anteriormente  \n",
    "    # Nesse exercício, você precisará usar apenas o hx retornado\n",
    "    # pela função forwardPropagation \n",
    "    z2, a2, z3, hx = forwardPropagation(X, Theta1, Theta2)\n",
    "\n",
    "    # inicializa o custo que você precisa calcular e retornar\n",
    "    J = 0; \n",
    "    \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #\n",
    "    # (1): use as saídas da rede (hx) obtidas com a função forwardPropagation e as classes\n",
    "    #      reais binarizadas (Ybin) para calcular o custo. \n",
    "    #      Obs: Você já fez isso no exercício anterior. Basta copiar de lá. \n",
    "    #\n",
    "    # (2): Implemente a regularização na função de custo.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "\n",
    "    return J, z2, a2, z3, hx\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos\n",
    "vLambda = 1;\n",
    "\n",
    "J, z2, a2, z3, hx = funcaoCusto_reg(X_train, Y_trainBin, initial_Theta1, initial_Theta2, vLambda)\n",
    "\n",
    "print('Custo obtidos: %1.6f ' %J)\n",
    "\n",
    "print('Custo esperado: 7.615168')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Nesta parte do exercício, você implementará o algoritmo de *backpropagation* responsável por calcular o gradiente para a função de custo da rede neural. Terminada a implementação do cálculo do gradiente, você poderá treinar a rede neural minimizando a função de custo $J(\\Theta)$ usando o algoritmo do gradiente descendente.\n",
    "\n",
    "Primeiro, você precisará implementar o gradiente para a rede neural sem regularização. Após ter verificado que o cálculo do gradiente está correto, você implementará o gradiente para a rede neural com regularização.\n",
    "\n",
    "Você deverá começar pela implementação do gradiente da sigmóide, o qual pode ser calculado utilizando a equação:\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)), $$\n",
    "\n",
    "sendo que\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}. $$\n",
    "\n",
    "Ao completar, teste diferentes valores para a função `sigmoidGradient`. Para valores grandes de *z* (tanto positivo, quanto negativo), o resultado deve ser próximo a zero. Quando $z = 0$, o resultado deve ser exatamente 0,25. A função deve funcionar com vetores e matrizes também. No caso de matrizes, a função deve calcular o gradiente para cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "\n",
    "    return g\n",
    "\n",
    "print('\\nAvaliando o gradiente da sigmoide...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([1,-0.5, 0, 0.5, 1]))\n",
    "print('Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\\n')\n",
    "print(g)\n",
    "\n",
    "print('\\nSe sua implementacao estiver correta, o gradiente da sigmoide sera:')\n",
    "print('[0.19661193 0.23500371 0.25 0.23500371 0.19661193]\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do algoritmo *backpropagation* é encontrar a \"parcela de responsabilidade\" que cada neurônio da rede neural teve com o erro gerado na saída. Dada uma amostra de treino ($x^{(t)}, y^{(t)}$), primeiro é executado o passo de *feedforward* para calcular todas as ativações na rede, incluindo o valor de saída $h_{\\Theta}(x)$. Posteriormente, para cada neurônio $j$ na camada $l$, é calculado o \"erro\" $\\delta_{j}^{(l)}$ que mede o quanto determinado neurônio contribuiu para a diferença entre o valor esperado e o valor obtido na saída da rede.\n",
    "\n",
    "Nos neurônios de saída, a diferença pode ser medida entre o valor esperado (rótulo da amostra) e o valor obtido (a ativação final da rede), onde tal diferença será usada para definir $\\delta_{j}^{(3)}$ (visto que a camada 3 é a última). Nas camadas ocultas (quando houver mais de uma), o termo $\\delta_{j}^{(l)}$ será calculado com base na média ponderada dos erros encontrados na camada posterior ($l + 1$).\n",
    "\n",
    "A seguir, é descrito em detalhes como a implementação do algoritmo *backpropagation* deve ser feita. A descrição abaixo considera que iremos seguir os passos 1 a 4 dentro de um laço, processando uma amostra por vez. Mas, é possível também executar esses passos sem laço de repetição, usando apenas com operações matriciais. No passo 5, o gradiente acumulado é dividido pelas *m* amostras, o qual será utilizado na função de custo da rede neural.\n",
    "\n",
    " 1. Coloque os valores na camada de entrada ($a^{(1)}$) para a amostra de treino a ser processada. Calcule as ativações das camadas 2 e 3 utilizando o passo de *feedforward*. Observe que será necessário adicionar um termo $+1$ para garantir que os vetores de ativação das camadas ($a^{(1)}$) e ($a^{(2)}$) também incluam o neurônio de *bias*.\n",
    " \n",
    " 2. Para cada neurônio $k$ na camada 3 (camada de saída), defina:\n",
    "    $$ \\delta_{k}^{(3)} = (a^{(3)}_k - y_k), $$\n",
    "    onde $y_k \\in \\{0,1\\}$ indica se a amostra sendo processada pertence a classe $k$ ($y_k = 1$) ou não ($y_k = 0$).\n",
    "    \n",
    " 3. Para a camada oculta $l$ = 2, defina:\n",
    "\n",
    "    $$ \\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)}*g'(z^{(2)}) $$\n",
    "    \n",
    " 4. Acumule o gradiente usando a fórmula descrita a seguir. Lembre-se de não utilizar o valor associado ao bias $\\delta^{(2)}_0$.\n",
    "    \n",
    "    $$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T $$\n",
    "    \n",
    " 5. Obtenha o gradiente sem regularização para a função de custo da rede neural dividindo os gradientes acumulados por $\\frac{1}{m}$:\n",
    " \n",
    "     $$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} $$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "Neste ponto, iremos implementar o algoritmo de *backpropagation*. Vamos criar uma nova função de custo, baseada na função implementada anteriormente, que retorna as derivadas parciais dos parâmetros. Nesta função, iremos implementar o gradiente para a rede neural sem regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_backp(Theta1, Theta2, X, Y):\n",
    "    '''\n",
    "    Calcula o custo e gradiente da rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao.\n",
    "\n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    Parametros\n",
    "    ----------   \n",
    "    X : matriz com os dados de treinamento\n",
    "    y : vetor com as classes dos dados de treinamento\n",
    "    Theta1: pesos que ligam a camada de entrada à camada intermediária\n",
    "    Theta2: pesos que ligam a camada intermediária à camada de saída\n",
    "    vLambda: parametro de regularizacao\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    J: valor do custo\n",
    "    \n",
    "    Theta1_grad: derivadas parciais dos pesos da primeira camada.\n",
    "    \n",
    "    Theta1_grad: derivadas parciais dos pesos da segunda camada.\n",
    "    '''\n",
    "\n",
    "    # estrai a quantidade de classes\n",
    "    num_labels = len(np.unique(Y))\n",
    "    \n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # retorna o custo J e as saídas de cada camada da rede (z2, a2, z3, hx)\n",
    "    J, z2, a2, z3, hx = funcaoCusto(X, Y, Theta1, Theta2)  \n",
    "    \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)  \n",
    "       \n",
    "    # executa a etapa backpropagation\n",
    "    # ------------------------------------------\n",
    "    \n",
    "    delta3 = hx-Y\n",
    "    \n",
    "    delta2 = np.dot( delta3,Theta2[:,1:] )\n",
    "    delta2 = delta2 * sigmoidGradient(z2)\n",
    "    \n",
    "    # a1 = x com uma coluna a mais com valor 1 para o bias\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    \n",
    "    # acumula o gradiente\n",
    "    delta_cap2 = np.dot(delta3.T, a2) \n",
    "    delta_cap1 = np.dot(delta2.T, a1)\n",
    "\n",
    "    # divide os gradientes acumulados por m\n",
    "    Theta1_grad = delta_cap1 / m \n",
    "    Theta2_grad = delta_cap2 / m \n",
    "    # ------------------------------------------ \n",
    "\n",
    "    return J, Theta1_grad, Theta2_grad\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 3;\n",
    "\n",
    "J, Theta1_grad, Theta2_grad = funcaoCusto_backp(initial_Theta1, initial_Theta2, X_train, Y_trainBin)\n",
    "\n",
    "print(Theta1_grad.shape)\n",
    "print(Theta2_grad.shape)\n",
    "\n",
    "print('Segunda derivada parcial da primeira camada: %1.6f' %Theta1_grad[5,0])\n",
    "\n",
    "print('Segunda derivada parcial da segunda camada: %1.6f' %Theta2_grad[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outra parte da regularização\n",
    "\n",
    "Agora, a regularização deverá ser adicionada após se calcular o gradiente durante o algoritmo de *backpropagation*. Lembre-se que a regularização não é adicionada quando $j = 0$, ou seja, na primeira coluna de $\\Theta$. Portanto, para $j \\geq 1$, o gradiente é descrito como:\n",
    "\n",
    "$$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\frac{\\lambda}{m}\\Theta^{(l)}_{ij} $$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Vamos criar uma nova função de custo que é uma atualização da função anterior, mas com regularização e gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_backp_reg(Theta1, Theta2, X, Y, vLambda):\n",
    "    '''\n",
    "    Calcula o custo e gradiente da rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao.\n",
    "\n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    Parametros\n",
    "    ----------   \n",
    "    X : matriz com os dados de treinamento\n",
    "    y : vetor com as classes dos dados de treinamento\n",
    "    Theta1: pesos que ligam a camada de entrada à camada intermediária\n",
    "    Theta2: pesos que ligam a camada intermediária à camada de saída\n",
    "    vLambda: parametro de regularizacao\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    J: valor do custo\n",
    "    \n",
    "    grad: vetor que contem todas as derivadas parciais\n",
    "          da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "    # retorna o custo J e as saídas de cada camada da rede (z2, a2, z3, hx)\n",
    "    J, z2, a2, z3, hx = funcaoCusto_reg(X, Y, Theta1, Theta2, vLambda) \n",
    "    \n",
    "    # backpropagation\n",
    "    # ------------------------------------------\n",
    "    \n",
    "    delta3 = hx-Y\n",
    "    \n",
    "    delta2 = np.dot( delta3,Theta2[:,1:] )\n",
    "    delta2 = delta2 * sigmoidGradient(z2) \n",
    "    \n",
    "    # acumula o gradiente\n",
    "    # --------------------------\n",
    "    delta_cap2 = np.dot(delta3.T, a2) \n",
    "    \n",
    "    # a1 = x com uma coluna a mais com valor 1 para o bias\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    \n",
    "    delta_cap1 = np.dot(delta2.T, a1)\n",
    "    # --------------------------\n",
    "\n",
    "\n",
    "    # divide os gradientes acumulados por m\n",
    "    Theta1_grad = delta_cap1 / m \n",
    "    Theta2_grad = delta_cap2 / m \n",
    "    \n",
    "    # acrescenta a regularização\n",
    "    # A regularização não pode ser adicionada no bias. Ou seja, a coluna 0 do Theta não deve ser usada.\n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + vLambda / m * Theta1[:,1:]\n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + vLambda / m * Theta2[:,1:]\n",
    "    # ------------------------------------------\n",
    "\n",
    "    return J, Theta1_grad, Theta2_grad\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 6;\n",
    "\n",
    "J, Theta1_grad, Theta2_grad = funcaoCusto_backp_reg(initial_Theta1, initial_Theta2, X_train, Y_trainBin, vLambda)\n",
    "\n",
    "print('Segunda derivada parcial da primeira camada: %1.6f' %Theta1_grad[5,0])\n",
    "\n",
    "print('Segunda derivada parcial da segunda camada: %1.6f' %Theta2_grad[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando a rede neural\n",
    "\n",
    "Neste ponto, todo o código necessário para treinar a rede está pronto.\n",
    "Agora, iremos iniciar o treinamento da rede neural usando o gradiente descendente para atualizar os pesos ao longo das épocas de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradienteDesc(X, Y, learnRate, MaxEpochs, vLambda, n_batches):\n",
    "\n",
    "    # inicializa dos thetas\n",
    "    Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "    Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)  \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    #\n",
    "    # 1) Faça o algoritmo executar até atingir o limite de épocas\n",
    "    # 2) Em cada época, execute um passo do gradiente usando mini-batch. Para separar\n",
    "    #    em batches, use o método array_split da biblioteca numpy.     \n",
    "    # 3) Use a função funcaoCusto_backp_reg para obter os gradientes\\\n",
    "    # 4) Atualize os thetas\n",
    "    # 5) Atualize o histórico\n",
    "    \n",
    "    for i in range(MaxEpochs):\n",
    "\n",
    "        if(i%100==0):\n",
    "            print(\"Epoca: %d\" % i)\n",
    "\n",
    "        X_batches = np.array_split(X, n_batches)\n",
    "        Y_batches = np.array_split(Y, n_batches)\n",
    "\n",
    "        for j in range(n_batches):\n",
    "\n",
    "            Xtemp = X_batches[j]\n",
    "            Ytemp = Y_batches[j] \n",
    "\n",
    "            J, Theta1_grad, Theta2_grad = funcaoCusto_backp_reg(Theta1, Theta2, Xtemp, Ytemp, vLambda)\n",
    "\n",
    "            Theta1 = Theta1 - (learnRate * Theta1_grad)\n",
    "            Theta2 = Theta2 - (learnRate * Theta2_grad)\n",
    "\n",
    "        ErrorHistory.append(J)   \n",
    "    \n",
    "    ##########################################################################\n",
    "    \n",
    "    return Theta1, Theta2, ErrorHistory\n",
    "    \n",
    "\n",
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxEpochs para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxEpochs = 2000\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1.5\n",
    "    \n",
    "ErrorHistory = []\n",
    "learnRate = 0.1\n",
    "n_batches=10\n",
    "\n",
    "Theta1, Theta2, errorHistory = gradienteDesc(X_train, Y_trainBin, learnRate, MaxEpochs, vLambda, n_batches)\n",
    "\n",
    "plt.plot(errorHistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando os pesos\n",
    "\n",
    "\n",
    "Uma das formas de entender o que a rede neural está aprendendo, é visualizar a representação capturada nos neurônios da camada oculta. Informalmente, dado um neurônio de uma camada oculta qualquer, uma das formas de visualizar o que esse neurônio calcula, é encontrar uma entrada *x* que o fará ser ativado (ou seja, um resultado próximo a 1). Para a rede neural que foi treinada, perceba que a $i$-ésima linha de $\\Theta^{(1)}$ é um vetor com 785 dimensões, o qual representa os parâmetros para o $i$-ésimo neurônio. Se descartarmos o termo *bias*, teremos um vetor de 784 dimensões que representa o peso para cada pixel a partir da camada de entrada. Deste modo, uma das formas de visualizar a representação capturada pelo neurônio da camada oculta, é reorganizar essas 784 dimensões em uma imagem de 28 x 28 pixels e exibi-la. \n",
    "\n",
    "O script a seguir irá exibir uma imagem com 25 unidades, cada uma correspondendo a um neurônio da camada oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nVisualizando a rede neural... \\n')\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta1[:, 1:].shape)\n",
    "\n",
    "visualizaDados(Theta1[:, 1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predição\n",
    "\n",
    "Após treinar a rede neural, ela será utilizada para predizer\n",
    "os rótulos das amostras. Neste ponto, foi implementada a função de predição\n",
    "para que a rede neural seja capaz de prever os rótulos no conjunto de dados\n",
    "e calcular a acurácia do método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao(X, Theta1, Theta2):\n",
    "    '''\n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    \n",
    "    num_labels = 10 # quantidade de classes\n",
    "        \n",
    "    # calcula as saídas da rede (hx)\n",
    "    z2, a2, z3, hx = forwardPropagation(X, Theta1, Theta2)\n",
    "    \n",
    "    # inicializa as predições\n",
    "    pred = np.zeros( m, dtype=int )\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #\n",
    "    # (1): use as saídas da rede (hx) obtidas com a função forwardPropagation e obtenha as classes. \n",
    "    #      Para cada dado, a classe deverá ser um dos seguintes valores: 0, 1, 2, 3, ..., 9\n",
    "    #  \n",
    "    # Obs: a classe predita pelo modelo é aquela que possui o maior valor de saída. Use o método\n",
    "    #      argmax do numpy \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################################################\n",
    "    \n",
    "    return pred\n",
    "   \n",
    "pred = predicao(X_test, Theta1, Theta2)\n",
    "\n",
    "print('\\nAcuracia no conjunto de teste: %1.2f%%\\n'%( np.mean( pred == Y_test ) * 100) )\n",
    "\n",
    "print('\\nAcuracia esperada: 89.80% (aproximadamente)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
